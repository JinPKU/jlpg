%!TEX program = xelatex

\documentclass[a4paper]{article}
%\documentclass{amsart}
\usepackage[breaklinks,linkcolor=black,citecolor=black,urlcolor=black]
{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{mathrsfs}
\usepackage[ruled]{algorithm2e}
\usepackage{subfigure}

%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[a4paper,left=2.5cm, right=2.5cm, top=2.5cm, 
bottom=2.5cm]{geometry}
\usepackage{xeCJK}
\usepackage{cleveref}

\newfontfamily{\con}{Consolas} 

%\lstset{language=Matlab}

\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{experiment}{Experiment}

%\numberwithin{equation}{section}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle} % inner-prod
\newcommand{\mr}{\mathbb{R}}
\newcommand{\mh}{\mathcal{H}}
\newcommand{\prox}{\mathrm{prox}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\proj}{\mathrm{Proj}}
\newcommand{\mi}{\mathbf{1}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\co}[1]{{\con{#1}}}

%\graphicspath{{../figures/}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}



%--------------------------------------------------------------------%
\title{Software implementation for the proximal gradient methods}
\author{Zeyu Jin, Ting Lin}
\date{\today}

\begin{document}
  \maketitle
  \tableofcontents
  
  \section{Introduction}
  \label{sec:intro}
  In this project, we consider the following composite optimization 
  problem
  \begin{equation}
    \min\limits_{x} F(x) = f(x) + \mu h(x),
  \end{equation}
  where $f(x)$ is differentiable, $h(x)$ is a function whose 
  proximal operator is easily available, $\mu > 0$ is a given 
  parameter.
  The proximal operator of $\mu h(x)$ is defined as
  \begin{displaymath}
    \prox_{\mu h}(x) = \mathop{\arg\min}\limits_{u} \mu h(u) + 
    \frac{1}{2} \norm{u - x}^2.
  \end{displaymath}
  Starting from a suitable initial point $x^0$, the proximal gradient 
  method is performed as
  \begin{equation}\label{equ:pg}
    x^{k+1} = \prox_{t_k h} (x^k - t_k \nabla f(x^k)),
  \end{equation}
  where $t_k$ is a chosen step size.
  
  In this project, we implement a general software package for the 
  proximal gradient method. Our software package has the following 
  features:
  \begin{itemize}
    \item Support the following objective functions:
    \begin{itemize}
      \item Least Square(both vector and matrix version): 
      \co{LS(A,b)}.
      \item Logistic regression: \co{LOGISTIC(A,b)}.
      \item We allow user-specified objective functions.
    \end{itemize}
  
    \item Support the following proximal pairs (See \cref{sec:cal} 
    for details):
    \begin{itemize}
      \item Vector form: \co{L1\_NORM, L2\_NORM, Linf\_NORM and 
      L0\_NORM
        L1\_BALL(R), L2\_NORM(R), Linf\_NORM(R)} and \co{L0\_NORM(R)}
      \item Matrix norm in generalized LASSO problem, currently 
      including \co{L12\_NORM} only.
      \item Spectral-relevant norm, currently including 
      \co{NUCLEAR\_NORM} only.
      \item \co{ELASTIC\_NET(lam)} and \co{LOG\_SUM}.
      \item Naive gradient method, use \co{NO\_PROX} or 
      \co{NO\_PROX\_MAT}.
      \item We allow user-specified proximal operators. In this way, 
      our software package supports the proximal operators in which 
      $x$ is divided into several blocks.
    \end{itemize}
    \item Support four kinds of optimization strategies to choose the 
    step size  (See \cref{sec:alg} for details):
    \begin{itemize}
      \item Constant: constant step size.
      \item Armijo: backtracking line search to achieve the Armijo 
      condition.
      \item Nonmonotone: backtracking line search to achieve a 
      non-monotone 
      condition using the BB step size.
      \item Classical: a classical strategy to choose the step size 
      in the 
      proximal gradient method (See the document for details).
    \end{itemize}
    \item Easy construction of new \co{grad\_pair} and 
    \co{prox\_pair}.
    \item Support a continuation strategy (See \cref{sec:alg} for 
    details).
  \end{itemize}
  Our other contributions include:
  \begin{itemize}
    \item We calculate the gradients for least 
    square and logistic regression. We also calculate several 
    proximal operators.
    \item We conduct several numerical experiments, including both 
    random data and real data to test the codes
  \end{itemize}

  The rest of this report is organized as follows. In \cref{sec:alg}, 
  we introduce some details of our algorithms. In \cref{sec:cal}, we 
  present several typical scenarios of $f(x)$ and $h(x)$. In 
  \cref{sec:num}, we conduct several numerical experiments. Detailed 
  description of the design of the packages are presented in 
  \cref{app:design}.
  
  
  
  % description of algorithms & interpretation of implementation?
  \section{Algorithms}
  \label{sec:alg}
  
  \subsection{Strategies for choosing the step size}
  In \cref{sec:intro}, we have already given the basic frame of the 
  proximal gradient method. One may notice from \cref{equ:pg} that we 
  need to determine the step size $t_k$ at each step. In our 
  implementations, the following four types of strategies for 
  choosing the step size are supported.
  \begin{itemize}
    \item \co{Constant}: constant step size: $t_k = t_0, \ \forall k 
    \in \mathbb{N}$.
    \item \co{Armijo}: backtracking line search to achieve the Armijo 
    condition. The step size $t_k$ is chosen such that
    \begin{displaymath}
      f(x^{k+1}) \le f(x^k) + \rho \ip{\nabla f(x^k)}{x^{k+1} - x^k},
    \end{displaymath}
    where $\rho \in (0, 1)$ is given beforehand.
    \item \co{Nonmonotone}: backtracking line search to achieve a 
    non-monotone condition using the BB step size. The step size 
    $t_k$ is chosen such that 
    \begin{displaymath}
      f(x^{k+1}) \le \max\left\{
      f(x^j) : j = \max\{0, k-M+1\},\ldots,k \right\} + \rho 
      \ip{\nabla f(x^k)}{x^{k+1} - x^k},
    \end{displaymath}
    where $\rho \in (0, 1)$ and $M > 0$ are given beforehand.
    \item \co{Classical}: a classical backtracking line search 
    strategy to choose the step size in the proximal gradient method. 
    The step size $t_k$ is chosen such that 
    \begin{displaymath}
      f(x^{k+1}) \le f(x^k) + \ip{\nabla f(x^k)}{x^{k+1}- x^k} + 
      \frac{1}{2t_k} \norm{x^{k+1} - x^k}^2.
    \end{displaymath}
  \end{itemize}
  For the last three strategies, we use the we adopt the Armijo 
  backtracking method and adopt BB rule to determine the initial 
  guess of the step size. 

  \subsection{Continuation strategy}
  Later, we may use the continuation strategy.
  The basic scheme of this strategy is presented in Algorithm 
  \ref{alg:cont}. This strategy may improve efficiency for some 
  particular types of $h(x)$ if $h(x)$ is not an indicator function.
  \begin{algorithm}[h]
    \caption{Continuation strategy.}
    \label{alg:cont}
    \LinesNumbered
    Given the factor $\eta \in (0,1)$, the initial value $\mu_0$ and 
    the starting point $x_0$. Let $t \leftarrow 0$.\\
    \While{$\mu_t \ne \mu$ or {the stopping criteria are not 
        satisfied}}{
      Solve the group LASSO problem with parameter $\mu_t$ and 
      initial 
      value $x_t$, and obtain the optimal point $x_{t+1}$.\\
      $\mu_{t + 1} \leftarrow \max\{\eta \mu_t, \mu\}$.\\
      $t \leftarrow t + 1$.
    }
  \end{algorithm}
  
%  \subsection{Theoretical analysis}
  
  
  
  
  
  \section{Several scenarios}
  \label{sec:cal}
  
  \subsection{Scenarios of $f(x)$}
  \label{sec:cal_f}
  In this part, we list several typical scenarios of $f(x)$ that are 
  supported in our codes. We give their gradient explicitly.
  
  \begin{itemize}
    \item {Least squares in vector form:} $f(x) = \frac{1}{2} 
    \norm{Ax - b}_2^2$, where $A \in \mr^{m \times n}$, $x \in 
    \mr^n$, 
    $b 
    \in \mr^m$. The gradient of $f(x)$ is given by %\footnote{In this 
    %report, we write the gradient in the same shape as $x$.}
    \begin{displaymath}
      \nabla f(x) = A^T (A x - b).
    \end{displaymath}
    
    \item {Least squares in matrix form:} $f(x) = \frac{1}{2} 
    \norm{Ax - b}_F^2$, where $A \in \mr^{m \times n}$, $x \in \mr^{n 
      \times l}$, $b \in \mr^{m \times l}$. The gradient of $f(x)$ is 
    given 
    by
    \begin{displaymath}
      \nabla f(x) = A^T  (A x - b).
    \end{displaymath}
    
    \item {Logistic regression:} 
    \begin{displaymath}
      f(x) = \frac{1}{m} \sum_{i = 1}^{m} \log(1 + \exp(- b_i a_i^T 
      x)),
    \end{displaymath}
    where $x \in \mr^n$, $A = (a_1, \ldots, a_m) \in \mr^{n \times 
      m}$, 
    $a_i = (a_{1,i}, \ldots, a_{n,i})^T \in \mr^n$,
    $b = (b_1, \ldots, b_m)^T \in \mr^m$. The gradient of $f(x)$ is 
    given 
    by
    \begin{displaymath}
      \nabla f(x) = - \frac{1}{m} \sum_{i = 1}^{m} \frac{b_i 
        a_i}{\exp(b_i a_i^T x) + 1}.
    \end{displaymath}
  \end{itemize}
  
  
  
  \subsection{Scenarios of $h(x)$}
  \label{sec:cal_h}
  In this part, we list several typical scenarios of $h(x)$ that are 
  supported in our codes. We give the proximal operator of $\mu h(x)$ 
  explicitly.
  
  \begin{itemize}
    \item {Vector $\ell_0$-norm:} $h(x) = \norm{x}_0 = \# \{i:x_i 
    \ne 0\}$, where $x \in \mr^n$. The proximal operator of $\mu 
    h(x)$ 
    is 
    given by
    \begin{displaymath}
      \prox_{\mu h}(x) = \mh_{\mu}(x_1) \times \ldots \times 
      \mh_{\mu}(x_n),
    \end{displaymath}
    where $\mh_{\mu}(s)$ is given by
    \begin{displaymath}
      \mh_{\mu}(s) = 
      \begin{cases}
        \{0\},    & |s| < \sqrt{2\mu}, \\
        \{0, s\}, & |s| = \sqrt{2\mu}, \\
        \{s\},    & |s| > \sqrt{2\mu}.
      \end{cases}
    \end{displaymath}
    
    \item {Vector $\ell_1$-norm:} $h(x) = \norm{x}_1 = 
    \sum_{i=1}^{n} |x_i|$, where $x \in \mr^n$. The proximal operator 
    of 
    $\mu h(x)$ is given by
    \begin{displaymath}
      \prox_{\mu h}(x) = (\mh_{\mu}(x_1), \ldots, \mh_{\mu}(x_n))^T,
    \end{displaymath}
    where $\mh_{\mu}(s)$ is given by
    \begin{displaymath}
      \mh_{\mu}(s) = \sign(s) \max\{ |s| - \mu, 0 \}.
    \end{displaymath}
    
    \item {Vector $\ell_2$-norm:} $h(x) = \norm{x}_2 = 
    \left( \sum_{i=1}^{n} |x_i|^2 \right)^{\frac{1}{2}}$, where $x 
    \in 
    \mr^n$. The proximal operator of $\mu h(x)$ is given by
    \begin{displaymath}
      \prox_{\mu h}(x) = 
      \begin{cases}
        \frac{\max\{ \norm{x}_2 - \mu, 0 \}}{\norm{x}_2} x, & x \ne 
        0, 
        \\
        0, & x = 0.
      \end{cases}
    \end{displaymath}
    
    
    \item {Vector $\ell_\infty$-norm:} $h(x) = \norm{x}_\infty = 
    \max\{ |x_i|: i = 1, 2, \ldots, n \}$, where $x \in 
    \mr^n$. By Moreau's decomposition, the proximal operator of $\mu 
    h(x)$ is given by
    \begin{displaymath}
      \begin{split}
        \prox_{\mu h}(x) & = x - \mu \; \prox_{\mu^{-1} h^\star} 
        \left( 
        \frac{x}{\mu} \right) \\
        & = x - \mu \; \proj_{B_{\norm{\cdot}_1}(1)} \left( 
        \frac{x}{\mu} \right),
      \end{split}
    \end{displaymath}
    where $h^\star(y) = \mi_{B_{\norm{\cdot}_1}(1)}(y)$, $\mi_C(y)$ 
    is 
    the indicator function defined by
    \begin{displaymath}
      \mi_C(y) = 
      \begin{cases}
        0, & y \in C, \\
        +\infty, & y \notin C,
      \end{cases}
    \end{displaymath}
    $B_{\norm{\cdot}}(r)$ is the norm ball defined by
    \begin{displaymath}
      B_{\norm{\cdot}}(r) = \left\{ y: \norm{y} \le r \right\},
    \end{displaymath}
    $\proj_C(x)$ is the projection operator defined by
    \begin{displaymath}
      \proj_C(x) = \mathop{\arg\min}\limits_{u \in C} \norm{u - x}_2.
    \end{displaymath}
    
    
    \item {Matrix $\ell_{1,2}$-norm:} $$h(x) = \norm{x}_{1,2} = 
    \sum_{i = 1}^{n} \norm{x(i,1:l)}_2, $$
    where $x \in \mr^{n \times l}$. The proximal operator of $\mu 
    h(x)$ 
    is given by
    \begin{displaymath}
      (\prox_{\mu h}(x))_{i,j} = 
      \begin{cases}
        x_{i,j} \max\left\{ 1 - \frac{\mu}{\norm{x(i,1:l)}_2}, 0 
        \right\}, & \norm{x(i,1:l)}_2 > 0, \\
        0, & \norm{x(i,1:l)}_2 = 0.
      \end{cases}
    \end{displaymath}
    
    
    %\item {Matrix $\ell_{2,1}$-norm:} $$h(x) = \norm{x}_{2,1} = 
    %  \left( \sum_{i = 1}^{n} \norm{x(i,1:l)}_1^2 
    %\right)^{\frac{1}{2}}, 
    %  $$
    %  where $x \in \mr^{n \times l}$. The proximal operator of $\mu 
    %h(x)$ 
    %  is given by
    
    \item {Nuclear norm:} $h(x) = \norm{x}_* = \sum_{i = 1}^{r} 
    \sigma_i(x)$, where $x \in \mr^{m \times n}$, $\{ \sigma_i(x) 
    \}_{i=1}^r$ are all the $r$ non-zero singular values of $x$. Let 
    the SVD of $x$ be $x = U \Sigma V^T$, where $\Sigma = 
    \diag_{m,n}\left\{ \sigma_1, \ldots, \sigma_r \right\}$ denotes 
    the 
    $m \times n$ matrix $\Sigma$ with $\Sigma_{i,i} = \sigma_i$ for 
    $i 
    = 1, 2, \ldots, r$ and other elements zero. Here $\sigma_1 \ge 
    \sigma_2 \ge \ldots \ge \sigma_r$. Let $d_i = \max\{ 
    \sigma_i - \mu, 0 \}$ and $D = \diag_{m,n}\left\{ d_1, \ldots, 
    d_r 
    \right\}$. The proximal operator of $\mu h(x)$ is given by
    \begin{displaymath}
      \prox_{\mu h}(x) = U D V^T.
    \end{displaymath}
    
    \item {Elastic-net:} $h(x) = \norm{x}_1 + \frac{\lambda}{2} 
    \norm{x}_2^2$, where $x \in \mr^n$. The proximal operator of $\mu 
    h(x)$ is given by
    \begin{displaymath}
      (\prox_{\mu h}(x))_i =  \frac{\sign(y_i) \; \max\{ |y_i| - \mu, 
        0 
        \}}{1 + \lambda \mu}.
    \end{displaymath}
    
    \item {Sum of logarithms:} $h(x) = -\sum_{i=1}^{n} \log(x_i)$, 
    where $x \in \mr^n$. The proximal operator of $\mu h(x)$ 
    is given by
    \begin{displaymath}
      (\prox_{\mu h}(x))_i = \frac{x_i + \sqrt{x_i^2 + 4\mu}}{2}.
    \end{displaymath}
    
    \item {Indicator of $\ell_0$-ball:} $h(x) = 
    \mi_{B_{\norm{\cdot}_0}(R)}$, where $x \in \mr^n$ and $R$ is a 
    positive integer. Let $\left\{ \sigma(1), \sigma(2), \ldots, 
    \sigma(n) \right\}$ be a permutation of $\left\{  1, 2, \ldots, n 
    \right\}$ such that 
    \begin{displaymath}
      |x_{\sigma(1)}| \ge |x_{\sigma(2)}| \ge \ldots \ge 
      |x_{\sigma(n)}|.
    \end{displaymath}
    The proximal operator of $\mu h(x)$ is given by
    \begin{displaymath}
      (\prox_{\mu h}(x))_{\sigma(i)} = 
      \begin{cases}
        x_{\sigma(i)}, & i = 1, 2, \ldots, R, \\
        0, & i = R + 1, R + 2, \ldots, 
        n.
      \end{cases}
    \end{displaymath}
  
    \item {Indicator of $\ell_1$ ball:} We refer to the algorithm of 
    \cite{l1ball} to implement the projection onto $\ell_1$ ball. 
  Here we omit the details. 
    
    \item {Indicator of $\ell_2$-ball:} $h(x) = 
    \mi_{ B_{\norm{\cdot}_2}(R)}$, where $x, y \in \mr^n$.
    The proximal operator of $\mu h(x)$ is given by
    \begin{displaymath}
      \prox_{\mu h}(x) = 
      \begin{cases}
        x, & \norm{x}_2 \le R, \\
        \frac{R}{\norm{x}_2}(x), & \norm{x}_2 > R.
      \end{cases}
    \end{displaymath}
    
    \item {Indicator of simple box:} $$h(x) = \mi_{[l_1,u_1] \times 
      \ldots \times [l_n,u_n]}(x),$$
    where $x \in \mr^n$. Here $l_i$ and $u_i$ are allowed to be 
    $-\infty$ and $+\infty$, respectively. The proximal operator of 
    $\mu h(x)$ is given by
    \begin{displaymath}
      (\prox_{\mu h}(x))_i = \min\{ u_i, \max\{l_i, x_i\} \}.
    \end{displaymath}
    
    \item {Indicator of rank ball:} $h(x) = \mi_{\rank(\cdot) \le 
      R}$, 
    where $x \in \mr^{m \times n}$ and $R$ is a positive integer. Let 
    the SVD of $x$ be $x = U \Sigma V^T$, where $\Sigma = 
    \diag_{m,n}\left\{ \sigma_1, \ldots, \sigma_r \right\}$. Let 
    \begin{displaymath}
      d_i = 
      \begin{cases}
        \sigma_i, & i = 1, 2, \ldots, R, \\
        0, & i = R+1, R+2, \ldots, r,
      \end{cases}
    \end{displaymath}
    and $D = \diag_{m,n}\left\{ d_1, \ldots, d_r \right\}$.
    The proximal operator of $\mu h(x)$ is given by
    \begin{displaymath}
      \prox_{\mu h}(x) = U D V^T.
    \end{displaymath}
    
  \end{itemize}
  
  
  
  
  
  
  \section{Numerical experiments}
  \label{sec:num}
  In this section, we conduct several numerical experiments to reveal 
  the power of our package. 
  
  \subsection{Basic setup}
  We compare between all the three line 
  search rules given in \cref{sec:alg}. We use \co{cont.} to denote 
  whether the continuation strategy is used. For each numerical 
  experiments, we record
  \begin{itemize}
    \item \co{iters}: the number of total iterations;
    \item \co{cputime}: total run time;
    \item \co{fval}: the function value at the found optimal point;
    \item \co{optimality}: the optimality measure at the point $x$ is 
    defined by 
    \begin{displaymath}
      \text{\co{optimality}} = \norm{x - \prox_{\mu h}\left( x - 
      \nabla 
      f(x) \right)}.
    \end{displaymath}
    \item \co{flag}: the flag is $1$ if the termination criteria are
    satisfied, or is $0$ if not.
  \end{itemize}
  The solver terminates if one of the following termination criteria 
  is satisfied:
  \begin{itemize}
    \item the change in the function value: $|F(x^{k+1} - F(x^k))| 
    \le \text{\co{ftol}}$;
    \item the optimality measure: $\text{\co{optimality}} \le 
    \text{\co{gtol}}$.
  \end{itemize}
  Here \co{ftol} and \co{gtol} are given beforehand. We also 
  terminates the optimization process if the number of iterations 
  exceed a given limit.
  
  \subsection{Several examples}
  We conduct the following numerical experiments for different types 
  of $f(x)$ and $h(x)$.
  \begin{enumerate}
    \item LASSO problem: \co{LS} and \co{L1\_NORM};
    \item group LASSO problem: \co{LS} and \co{L12\_NORM};
    \item log-barrier problem: \co{LS} and \co{SUM\_LOG};
    \item logistic regression with sparsity: \co{LOGISTIC} and 
    \co{L1\_NORM};
    \item blocked norm ball constraint LS: \co{LS} and \co{my\_prox} 
    defined by combining \co{L2\_BALL} block and \co{Linf\_BALL} 
    block;
    \item simple box constrainted optimization: \co{my\_grad} 
    specified by users, and \co{BOX};
    \item nuclear norm: \co{LS} and \co{NUCLEAR\_NORM};
    \item least square in $\ell_1$ ball: \co{LS} and \co{L1\_BALL};
    \item least square in $\ell_\infty$ ball: \co{LS} and 
    \co{Linf\_BALL};
    \item read data from Online Joke Recommender System.\footnote{The 
    \co{load\_csv} function refers from 
      \url{https://stackoverflow.com/questions/34247057/how-to-read-csv-file-and-assign-to-eigen-matrix}.}
  \end{enumerate}
  The numerical results are given in 
  \cref{tab1,tab2,tab3,tab4,tab5,tab6,tab7,tab8,tab9,tab10}.
  
  
  
  
  
  \begin{table}[p]
    \centering
    \begin{tabular}{cc|ccccc}
      \hline
      type & cont. & iters & cputime & 
      fval & optimality & flag \\ \hline
      Armijo & 1 & 135 & 0.09375 & 0.223574 & 0.00367652 & 1 \\
      Nonmonotone & 1 & 114 & 0.046875 & 0.223573 & 2.34637e-05 & 1 \\
      Classical & 1 & 109 & 0.046875 & 0.223573 & 3.50546e-05 & 1 \\
      \hline
    \end{tabular}
    \caption{Numerical results for Example 1.}
    \label{tab1}
  \end{table}

  
  \begin{table}[p]
    \centering
    \begin{tabular}{cc|ccccc}
      \hline
      type & cont. & iters & cputime & 
      fval & optimality & flag \\ \hline
      Armijo & 1 & 198 & 0.3125 & 0.178611 & 0.0218023 & 1 \\
      Nonmonotone & 1 & 327 & 0.28125 & 0.173687 & 0.000139197 & 1 \\
      Classical & 1 & 334 & 0.28125 & 0.173687 & 5.51511e-05 & 1 \\
      \hline
    \end{tabular}
    \caption{Numerical results for Example 2.}
    \label{tab2}
  \end{table}

  
%    In this numerical example, we test the group LASSO problem.
%    In other words, the types of $f(x)$ and $h(x)$ are \co{LS} and 
%    \co{L12\_NORM}, respectively. Let the elements of $A \in \mr^{m 
%      \times n}$ and $u \in \mr^{n \times l}$ be uniformly random 
%    numbers from $[0,1]$. Let $b = Au$. We take $m = 128$, $n = 256$ 
%    and $l = 2$. The initial value is $x^0 = 0$. Let $\mu = 0.01$. 
%We 
%    use continuation strategy and compare between three 
%    types of line search methods. The numerical results are 
%presented 
%    in \cref{tab3}.
  
  
  \begin{table}[p]
    \centering
    \begin{tabular}{cc|ccccc}
      \hline
      type & cont. & iters & cputime & 
      fval & optimality & flag \\ \hline
      Armijo & 1 & 29 & 0 & 4.86937 & 0.0201708 & 1 \\
      Nonmonotone & 1 & 37 & 0 & 4.86934 & 0.000387912 & 1 \\
      Classical & 1 & 54 & 0 & 4.86934 & 0.000220955 & 1 \\
      \hline
    \end{tabular}
    \caption{Numerical results for Example 3.}
    \label{tab3}
  \end{table}


  %    In this numerical example, we test the group LASSO problem.
  %    In other words, the types of $f(x)$ and $h(x)$ are \co{LS} and 
  %    \co{L12\_NORM}, respectively. Let the elements of $A \in 
  %\mr^{m 
  %      \times n}$ and $u \in \mr^{n \times l}$ be uniformly random 
  %    numbers from $[0,1]$. Let $b = Au$. We take $m = 128$, $n = 
  %256$ 
  %    and $l = 2$. The initial value is $x^0 = 0$. Let $\mu = 0.01$. 
  %We 
  %    use continuation strategy and compare between three 
  %    types of line search methods. The numerical results are 
  %presented 
  %    in \cref{tab3}.


\begin{table}[p]
  \centering
  \begin{tabular}{cc|ccccc}
    \hline
    type & cont. & iters & cputime & 
    fval & optimality & flag \\ \hline
    Armijo & 1 & 258 & 0 & 0.0258146 & 7.61302e-05 & 1 \\
    Nonmonotone & 1 & 666 & 0 & 0.0253084 & 2.88564e-05 & 1 \\
    Classical & 1 & 792 & 0 & 0.0253074 & 3.99053e-06 & 1 \\
    \hline
  \end{tabular}
  \caption{Numerical results for Example 4.}
  \label{tab4}
\end{table}


  %    In this numerical example, we test the group LASSO problem.
  %    In other words, the types of $f(x)$ and $h(x)$ are \co{LS} and 
  %    \co{L12\_NORM}, respectively. Let the elements of $A \in 
  %\mr^{m 
  %      \times n}$ and $u \in \mr^{n \times l}$ be uniformly random 
  %    numbers from $[0,1]$. Let $b = Au$. We take $m = 128$, $n = 
  %256$ 
  %    and $l = 2$. The initial value is $x^0 = 0$. Let $\mu = 0.01$. 
  %We 
  %    use continuation strategy and compare between three 
  %    types of line search methods. The numerical results are 
  %presented 
  %    in \cref{tab3}.


\begin{table}[p]
  \centering
  \begin{tabular}{cc|ccccc}
    \hline
    type & cont. & iters & cputime & 
    fval & optimality & flag \\ \hline
    Armijo & 1 & 66 & 0 & 0.0523895 & 0.000105485 & 1 \\
    Nonmonotone & 1 & 68 & 0 & 0.0523895 & 0.000147343 & 1 \\
    Classical & 1 & 81 & 0 & 0.0523896 & 0.000307073 & 1 \\
    \hline
  \end{tabular}
  \caption{Numerical results for Example 5.}
  \label{tab5}
\end{table}
  
  

  
  \begin{table}[p]
    \centering
    \begin{tabular}{cc|ccccc}
      \hline
      type & cont. & iters & cputime & 
      fval & optimality & flag \\ \hline
      Armijo & 1 & 3 & 0 & 1.88988 & 2.05963e-08 & 1 \\
      Nonmonotone & 1 & 3 & 0 & 1.88988 & 2.05963e-08 & 1 \\
      Classical & 1 & 10 & 0 & 1.88988 & 2.67976e-09 & 1 \\
      \hline
    \end{tabular}
    \caption{Numerical results for Example 6.}
    \label{tab6}
  \end{table}


  %    In this numerical example, we test the group LASSO problem.
  %    In other words, the types of $f(x)$ and $h(x)$ are \co{LS} 
  %and 
  %    \co{L12\_NORM}, respectively. Let the elements of $A \in 
  %\mr^{m 
  %      \times n}$ and $u \in \mr^{n \times l}$ be uniformly 
  %random 
  %    numbers from $[0,1]$. Let $b = Au$. We take $m = 128$, $n = 
  %256$ 
  %    and $l = 2$. The initial value is $x^0 = 0$. Let $\mu = 
  %0.01$. 
  %We 
  %    use continuation strategy and compare between three 
  %    types of line search methods. The numerical results are 
  %presented 
  %    in \cref{tab3}.


\begin{table}[p]
  \centering
  \begin{tabular}{cc|ccccc}
    \hline
    type & cont. & iters & cputime & 
    fval & optimality & flag \\ \hline
    Armijo & 1 & 74 & 0.546875 & 815.136 & 0.00171295 & 1 \\
    Nonmonotone & 1 & 151 & 0.703125 & 815.136 & 0.000385093 & 1 \\
    Classical & 1 & 191 & 0.96875 & 815.136 & 0.000632897 & 1 \\
    \hline
  \end{tabular}
  \caption{Numerical results for Example 7.}
  \label{tab7}
\end{table}
  
  
  
  
    %    In this numerical example, we test the group LASSO problem.
    %    In other words, the types of $f(x)$ and $h(x)$ are \co{LS} 
    %and 
    %    \co{L12\_NORM}, respectively. Let the elements of $A \in 
    %\mr^{m 
    %      \times n}$ and $u \in \mr^{n \times l}$ be uniformly 
    %random 
    %    numbers from $[0,1]$. Let $b = Au$. We take $m = 128$, $n = 
    %256$ 
    %    and $l = 2$. The initial value is $x^0 = 0$. Let $\mu = 
    %0.01$. 
    %We 
    %    use continuation strategy and compare between three 
    %    types of line search methods. The numerical results are 
    %presented 
    %    in \cref{tab3}.
  
  
  \begin{table}[p]
    \centering
    \begin{tabular}{cc|ccccc}
      \hline
      type & cont. & iters & cputime & 
      fval & optimality & flag \\ \hline
      Armijo & 0 & 14 & 0 & 2016.87 & 1.01978e-05 & 1 \\
      Nonmonotone & 0 & 14 & 0 & 2016.87 & 1.01978e-05 & 1 \\
      Classical & 0 & 14 & 0 & 2016.87 & 1.01978e-05 & 1 \\
      \hline
    \end{tabular}
    \caption{Numerical results for Example 8.}
    \label{tab8}
  \end{table}


  %    In this numerical example, we test the group LASSO problem.
  %    In other words, the types of $f(x)$ and $h(x)$ are \co{LS} 
  %and 
  %    \co{L12\_NORM}, respectively. Let the elements of $A \in 
  %\mr^{m 
  %      \times n}$ and $u \in \mr^{n \times l}$ be uniformly 
  %random 
  %    numbers from $[0,1]$. Let $b = Au$. We take $m = 128$, $n = 
  %256$ 
  %    and $l = 2$. The initial value is $x^0 = 0$. Let $\mu = 
  %0.01$. 
  %We 
  %    use continuation strategy and compare between three 
  %    types of line search methods. The numerical results are 
  %presented 
  %    in \cref{tab3}.


\begin{table}[p]
  \centering
  \begin{tabular}{cc|ccccc}
    \hline
    type & cont. & iters & cputime & 
    fval & optimality & flag \\ \hline
    Armijo & 0 & 42 & 0 & 0.00905712 & 0.00505199 & 1 \\
    Nonmonotone & 0 & 77 & 0 & 0.00849407 & 0.00280368 & 1 \\
    Classical & 0 & 102 & 0 & 0.00845289 & 9.38481e-05 & 1 \\
    \hline
  \end{tabular}
  \caption{Numerical results for Example 9.}
  \label{tab9}
\end{table}


  %    In this numerical example, we test the group LASSO problem.
  %    In other words, the types of $f(x)$ and $h(x)$ are \co{LS} 
  %and 
  %    \co{L12\_NORM}, respectively. Let the elements of $A \in 
  %\mr^{m 
  %      \times n}$ and $u \in \mr^{n \times l}$ be uniformly 
  %random 
  %    numbers from $[0,1]$. Let $b = Au$. We take $m = 128$, $n = 
  %256$ 
  %    and $l = 2$. The initial value is $x^0 = 0$. Let $\mu = 
  %0.01$. 
  %We 
  %    use continuation strategy and compare between three 
  %    types of line search methods. The numerical results are 
  %presented 
  %    in \cref{tab3}.


\begin{table}[p]
  \centering
  \begin{tabular}{cc|ccccc}
    \hline
    type & cont. & iters & cputime & 
    fval & optimality & flag \\ \hline
    Armijo & 1 & 13 & 61.3945 & 1.19394e+06 & 7.46187 & 1 \\
    Nonmonotone & 1 & 52 & 101.36 & 1.13583e+06 & 0.0960994 & 1 \\
    Classical & 1 & 69 & 128.949 & 1.13583e+06 & 0.0866642 & 1 \\
    \hline
  \end{tabular}
  \caption{Numerical results for Example 10.}
  \label{tab10}
\end{table}
  
  
  %\bibliographystyle{plain}
  %\bibliography{ref}
  
  
  \appendix
  \renewcommand{\appendixname}{Appendix~\Alph{section}}
  
  %\newpage
  
  \section{Design of the packages}
  \label{app:design}
  
  \subsection{Structure of our package}
  The structure of the folder \co{code/} is listed as follows:
  \begin{itemize}
    \item \co{include/} contains header file of our jlpg package, 
    including 
    \begin{itemize}
      \item \co{jlpg.hpp} the wrapper of our all header file.
      \item \co{funcpairs.hpp} define useful function pairs like 
      \co{LS}, \co{L1\_NORM}, \co{L2\_NORM}.
      \item \co{problem.hpp}: the construction and basic function of 
      the objective.
      \item \co{solver.hpp} the solver of our proximal gradient 
      method.
      \item \co{continuation.hpp} provides an interface to use 
      continuation method accelerating our program for those $h(x)$ 
      that are not indicator functions.
    \end{itemize}
    
    \item \co{example/} contains some examples we create in order to 
    reveal the power of our package. For example, 
    \begin{itemize}
      \item \co{ex1.cpp}: the lasso problem of size $256 \times 512$.
      \item \co{ex2.cpp}: the group lasso problem.
      \item $\ldots$
    \end{itemize}
  
    \item Inside \co{code/} dictionary, there is also a README file, 
    introducing the basic framework of our package and some advanced 
    parameters or settings.
  \end{itemize}

  \subsection{Dependency}
  \begin{itemize}
    \item Need C++11 support, G++(>=5.0) is recommended. It is 
    welcome to inform us that the performance under other compile 
    environments.
    \item Eigen(>=3.0) is required.
  \end{itemize}

  \subsection{An illustrative example}
  The steps to use our package are given by an illustrative example 
  as follows.
  \begin{enumerate}
    \item Include the necessary files.
    \begin{lstlisting}[language=C++,style=mystyle]
      #include <iostream>  
      // for IO
      #include <eigen3/Eigen/Dense> 
      // for using eigen
      #include "jlpg.hpp"
      // our package, change to the right file
      using namespace std;
    \end{lstlisting}
  
    \item Create Data for $A$ and $b$.
    \begin{lstlisting}[language=C++,style=mystyle]
      Mat A(3,3); // Mat is MatrixXd in eigen.
      A << 1,2,3,4,5,6,7,8,9; // Assign value
      Vec b(3);  // Vec is VectorXd in eigen
      b << 1,4,9; // Assign Value
    \end{lstlisting}
  
    \item Set up the problem:
    \begin{displaymath}
      \min\limits_{x} \frac{1}{2} \norm{Ax - b}_2^2 + 0.01 \norm{x}_1
    \end{displaymath}
    \begin{lstlisting}[language=C++,style=mystyle]
      // set up the problem with LeastSqaure and L1 norm.
      Problem<Vec> p(LS(A,b),L1_NORM) 
      // set up the coefficient of problem
      p.mu = 0.01; 
    \end{lstlisting}
  
    \item Set up options for solver.
    \begin{lstlisting}[language=C++,style=mystyle]
      // Create the option
      Options opts(10000, 1e-8, 1e-6, 1e-0, 5e-1); 
      // set strategy for the option: classical
      opts.setClassical(); 
    \end{lstlisting}
    
    \item Solve it happily.
    \begin{lstlisting}[language=C++,style=mystyle]
      Outputs out; //output structure
      Vec x(3); x << 0,0,0; //init value
      x = pgm(p,x,opts,out);
      cout << x << endl;
    \end{lstlisting}
  
    \item Suitable compile command: \\\co{g++ -O3 -march=native 
    -std=c++11 naivelasso -o naivelasso.out -I../include -DVERBOSED=1 
    }
  \end{enumerate}
  
  \begin{thebibliography}{1}
    \bibitem{l1ball} Condat, Laurent. "Fast projection onto the 
    simplex 
    and the $\pmb {l} _\mathbf {1} $ ball." Mathematical Programming 
    158.1-2 (2016): 575-585.
  \end{thebibliography}
  
\end{document}
